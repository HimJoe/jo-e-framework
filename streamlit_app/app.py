"""
Jo.E Framework Streamlit App

This Streamlit application provides a user interface for the Joint Evaluation (Jo.E) framework,
allowing users to evaluate AI systems using the tiered evaluation pipeline with LLMs,
AI agents, and human expertise.
"""

import os
import sys
import json
import logging
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List, Any, Optional

# Add parent directory to path so we can import from framework
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import Jo.E framework components
from framework.joe_framework import JoEFramework
from framework.llm_evaluators.evaluator_llm import GPT4Evaluator, Llama3Evaluator
from framework.ai_agents.agent_evaluator import (
    AdversarialAgent, BiasDetectionAgent, 
    KnowledgeVerificationAgent, EthicalBoundaryAgent
)
from framework.human_evaluation.human_review import HumanReview
from framework.scoring.scoring_system import ScoringSystem

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("streamlit_app.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Set page config
st.set_page_config(
    page_title="Jo.E Framework",
    page_icon="ðŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if "initialized" not in st.session_state:
    st.session_state.initialized = False
    st.session_state.framework = None
    st.session_state.evaluation_results = {}
    st.session_state.models = []
    st.session_state.datasets = {}
    st.session_state.api_keys = {}

# Visualization and helper functions

def load_sample_datasets() -> Dict[str, List[Dict[str, Any]]]:
    """
    Load sample datasets for testing and demonstration.
    
    Returns:
        Dictionary mapping dataset names to lists of examples
    """
    # Path to sample datasets
    base_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "datasets", "samples")
    
    # Dataset types
    dataset_files = {
        "General Knowledge": "general_knowledge.json",
        "Reasoning": "reasoning.json",
        "Creative": "creative.json",
        "Sensitive": "sensitive.json"
    }
    
    datasets = {}
    
    for name, filename in dataset_files.items():
        try:
            path = os.path.join(base_path, filename)
            with open(path, 'r') as f:
                data = json.load(f)
            datasets[name] = data
            logger.info(f"Loaded {len(data)} examples from {name} dataset")
        except FileNotFoundError:
            logger.warning(f"Dataset file not found: {path}")
            # Create a placeholder dataset
            datasets[name] = [
                {"id": f"{name.lower()}_1", "prompt": f"Example prompt for {name.lower()}"},
                {"id": f"{name.lower()}_2", "prompt": f"Another example for {name.lower()}"}
            ]
        except Exception as e:
            logger.error(f"Error loading {name} dataset: {e}")
            datasets[name] = []
    
    return datasets

def display_evaluation_results(evaluation_results: Dict[str, Any]) -> None:
    """
    Display evaluation results for a single output.
    
    Args:
        evaluation_results: Evaluation results dictionary
    """
    scores = evaluation_results["scores"]
    
    # Display scores
    st.success(f"Evaluation completed successfully. Jo.E Score: {scores['joe_score']:.2f}/5.0")
    
    # Create a score summary table
    score_df = pd.DataFrame({
        "Dimension": ["Accuracy", "Robustness", "Fairness", "Ethics", "Overall Jo.E Score"],
        "Score": [
            scores["accuracy_score"],
            scores["robustness_score"],
            scores["fairness_score"],
            scores["ethics_score"],
            scores["joe_score"]
        ]
    })
    
    st.write("### Score Summary")
    st.dataframe(score_df)
    
    # Create score visualization
    st.write("### Score Visualization")
    col1, col2 = st.columns(2)
    
    with col1:
        create_radar_chart(scores)
    
    with col2:
        create_bar_chart(scores)
    
    # Show evaluation summary
    st.write("### Evaluation Summary")
    
    summary = {
        "Total outputs evaluated": evaluation_results["total_outputs_evaluated"],
        "Outputs flagged for agent testing": evaluation_results.get("flagged_outputs_count", 0),
        "Verified issues": evaluation_results.get("verified_issues_count", 0)
    }
    
    for key, value in summary.items():
        st.metric(key, value)
    
    # Expandable detailed results
    if st.checkbox("Show Detailed Results"):
        evaluation_tabs = st.tabs(["LLM Evaluation", "Agent Testing", "Human Review"])
        
        # LLM Evaluation results
        with evaluation_tabs[0]:
            if "llm_results" in evaluation_results:
                llm_results = evaluation_results["llm_results"]
                st.write(f"LLM evaluation completed for {len(llm_results)} outputs.")
                
                for output_id, eval_result in llm_results.items():
                    with st.expander(f"Output {output_id}"):
                        st.write("**Scores:**")
                        for key, value in eval_result.items():
                            if key.endswith("_score"):
                                st.write(f"- {key.replace('_score', '').capitalize()}: {value}")
            else:
                st.info("No LLM evaluation results available.")
        
        # Agent Testing results
        with evaluation_tabs[1]:
            if "agent_results" in evaluation_results:
                agent_results = evaluation_results["agent_results"]
                st.write(f"Agent testing completed for {len(agent_results)} outputs.")
                
                for output_id, results in agent_results.items():
                    with st.expander(f"Output {output_id}"):
                        st.json(results)
            else:
                st.info("No agent testing results available.")
        
        # Human Review results
        with evaluation_tabs[2]:
            if "human_results" in evaluation_results and evaluation_results["human_results"]:
                human_results = evaluation_results["human_results"]
                st.write(f"Human review completed for {len(human_results)} outputs.")
                
                for output_id, review in human_results.items():
                    with st.expander(f"Output {output_id}"):
                        st.json(review)
            else:
                st.info("No human review results available.")

def display_batch_evaluation_summary(evaluation_results: Dict[str, Any]) -> None:
    """
    Display a summary of batch evaluation results.
    
    Args:
        evaluation_results: Evaluation results dictionary
    """
    scores = evaluation_results["scores"]
    
    # Display success message with Jo.E score
    st.success(f"Batch evaluation completed successfully. Jo.E Score: {scores['joe_score']:.2f}/5.0")
    
    # Create a score summary table
    score_df = pd.DataFrame({
        "Dimension": ["Accuracy", "Robustness", "Fairness", "Ethics", "Overall Jo.E Score"],
        "Score": [
            scores["accuracy_score"],
            scores["robustness_score"],
            scores["fairness_score"],
            scores["ethics_score"],
            scores["joe_score"]
        ]
    })
    
    st.write("### Score Summary")
    st.dataframe(score_df)
    
    # Create score visualization
    st.write("### Score Visualization")
    col1, col2 = st.columns(2)
    
    with col1:
        create_radar_chart(scores)
    
    with col2:
        create_bar_chart(scores)
    
    # Show evaluation summary
    st.write("### Evaluation Summary")
    
    summary = {
        "Total outputs evaluated": evaluation_results["total_outputs_evaluated"],
        "Outputs flagged for agent testing": evaluation_results.get("flagged_outputs_count", 0),
        "Verified issues": evaluation_results.get("verified_issues_count", 0)
    }
    
    summary_df = pd.DataFrame({
        "Metric": list(summary.keys()),
        "Count": list(summary.values())
    })
    
    st.table(summary_df)
    
    # Show pipeline efficiency
    if "flagged_outputs_count" in evaluation_results and "verified_issues_count" in evaluation_results:
        col1, col2, col3 = st.columns(3)
        
        total = evaluation_results["total_outputs_evaluated"]
        flagged = evaluation_results["flagged_outputs_count"]
        verified = evaluation_results["verified_issues_count"]
        
        llm_efficiency = (total - flagged) / total * 100
        agent_efficiency = (flagged - verified) / flagged * 100 if flagged > 0 else 0
        
        with col1:
            st.metric("LLM Filtering Efficiency", f"{llm_efficiency:.1f}%", 
                      help="Percentage of outputs filtered by LLM evaluation")
        
        with col2:
            st.metric("Agent Filtering Efficiency", f"{agent_efficiency:.1f}%", 
                     help="Percentage of flagged outputs filtered by agent testing")
        
        with col3:
            human_review_percentage = verified / total * 100
            st.metric("Human Review Required", f"{human_review_percentage:.1f}%", 
                     help="Percentage of total outputs requiring human review")
    
    # Expandable detailed results button
    if st.checkbox("Show Detailed Batch Results"):
        st.write("### Detailed Results")
        
        evaluation_tabs = st.tabs(["LLM Evaluation", "Agent Testing", "Human Review"])
        
        # LLM Evaluation results
        with evaluation_tabs[0]:
            if "llm_results" in evaluation_results:
                llm_results = evaluation_results["llm_results"]
                st.write(f"LLM evaluation completed for {len(llm_results)} outputs.")
                
                # Calculate average scores
                avg_scores = {}
                for output_id, eval_result in llm_results.items():
                    for key, value in eval_result.items():
                        if key.endswith("_score") and isinstance(value, (int, float)):
                            if key not in avg_scores:
                                avg_scores[key] = []
                            avg_scores[key].append(value)
                
                # Display average scores
                st.write("#### Average LLM Evaluation Scores")
                avg_df = pd.DataFrame({
                    "Metric": [key.replace("_score", "").capitalize() for key in avg_scores.keys()],
                    "Average Score": [sum(scores) / len(scores) for scores in avg_scores.values()]
                })
                
                st.dataframe(avg_df)
                
                # Show a sample of results
                st.write("#### Sample Results")
                sample_size = min(5, len(llm_results))
                sample_ids = list(llm_results.keys())[:sample_size]
                
                for output_id in sample_ids:
                    with st.expander(f"Output {output_id}"):
                        st.json(llm_results[output_id])
            else:
                st.info("No LLM evaluation results available.")
        
        # Agent Testing results
        with evaluation_tabs[1]:
            if "agent_results" in evaluation_results:
                agent_results = evaluation_results["agent_results"]
                st.write(f"Agent testing completed for {len(agent_results)} outputs.")
                
                # Calculate issue statistics
                issue_types = {}
                for output_id, results in agent_results.items():
                    for key, value in results.items():
                        if isinstance(value, dict) and "issue_detected" in value and value["issue_detected"]:
                            issue_type = value.get("issue_type", "other")
                            if issue_type not in issue_types:
                                issue_types[issue_type] = 0
                            issue_types[issue_type] += 1
                
                # Display issue statistics
                if issue_types:
                    st.write("#### Issue Types Detected")
                    issue_df = pd.DataFrame({
                        "Issue Type": list(issue_types.keys()),
                        "Count": list(issue_types.values())
                    })
                    
                    st.dataframe(issue_df)
                else:
                    st.info("No issues detected by AI agents.")
                
                # Show a sample of results
                st.write("#### Sample Results")
                sample_size = min(5, len(agent_results))
                sample_ids = list(agent_results.keys())[:sample_size]
                
                for output_id in sample_ids:
                    with st.expander(f"Output {output_id}"):
                        st.json(agent_results[output_id])
            else:
                st.info("No agent testing results available.")
        
        # Human Review results
        with evaluation_tabs[2]:
            if "human_results" in evaluation_results and evaluation_results["human_results"]:
                human_results = evaluation_results["human_results"]
                st.write(f"Human review completed for {len(human_results)} outputs.")
                
                # Calculate consensus statistics
                consensus_counts = {}
                for output_id, review in human_results.items():
                    if "summary" in review and "consensus" in review["summary"]:
                        consensus = review["summary"]["consensus"]
                        if consensus not in consensus_counts:
                            consensus_counts[consensus] = 0
                        consensus_counts[consensus] += 1
                
                # Display consensus statistics
                if consensus_counts:
                    st.write("#### Human Review Consensus")
                    consensus_df = pd.DataFrame({
                        "Consensus": list(consensus_counts.keys()),
                        "Count": list(consensus_counts.values())
                    })
                    
                    st.dataframe(consensus_df)
                
                # Show a sample of results
                st.write("#### Sample Results")
                sample_size = min(5, len(human_results))
                sample_ids = list(human_results.keys())[:sample_size]
                
                for output_id in sample_ids:
                    with st.expander(f"Output {output_id}"):
                        st.json(human_results[output_id])
            else:
                st.info("No human review results available.")

def create_radar_chart(scores: Dict[str, float]) -> None:
    """
    Create a radar chart for the evaluation scores.
    
    Args:
        scores: Dictionary containing scores
    """
    # Extract the scores
    dimensions = ["accuracy", "robustness", "fairness", "ethics"]
    values = [scores.get(f"{dim}_score", 0) for dim in dimensions]
    
    # Create figure and axis
    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
    
    # Number of dimensions
    N = len(dimensions)
    
    # Angle for each dimension
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]  # Close the loop
    
    # Add the values
    values += values[:1]  # Close the loop
    
    # Plot the values
    ax.plot(angles, values, linewidth=2, linestyle='solid')
    ax.fill(angles, values, alpha=0.25)
    
    # Set the labels
    dimension_labels = [dim.capitalize() for dim in dimensions]
    plt.xticks(angles[:-1], dimension_labels)
    
    # Set y-axis limits
    ax.set_ylim(0, 5)
    
    # Add title
    plt.title("Evaluation Scores", size=14)
    
    # Adjust layout
    plt.tight_layout()
    
    # Display the chart
    st.pyplot(fig)

def create_bar_chart(scores: Dict[str, float]) -> None:
    """
    Create a bar chart for the evaluation scores.
    
    Args:
        scores: Dictionary containing scores
    """
    # Extract the scores
    dimensions = ["accuracy", "robustness", "fairness", "ethics", "joe"]
    values = [scores.get(f"{dim}_score", 0) for dim in dimensions]
    
    # Create figure and axis
    fig, ax = plt.subplots(figsize=(8, 5))
    
    # Create the bar chart
    bars = ax.bar(range(len(dimensions)), values, color=sns.color_palette("viridis", len(dimensions)))
    
    # Add labels and title
    dimension_labels = [dim.capitalize() for dim in dimensions]
    dimension_labels[-1] = "Jo.E Score"  # Change 'Joe' to 'Jo.E Score'
    
    ax.set_xticks(range(len(dimensions)))
    ax.set_xticklabels(dimension_labels)
    ax.set_ylabel("Score (out of 5)")
    ax.set_title("Evaluation Scores by Dimension")
    
    # Set y-axis limits
    ax.set_ylim(0, 5.5)
    
    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2., height + 0.1,
                f"{height:.1f}", ha='center', va='bottom')
    
    # Add a light grid
    ax.yaxis.grid(True, linestyle='--', alpha=0.7)
    
    # Adjust layout
    plt.tight_layout()
    
    # Display the chart
    st.pyplot(fig)

def create_comparison_charts(scores_data: Dict[str, Dict[str, float]]) -> None:
    """
    Create comparison charts for multiple models.
    
    Args:
        scores_data: Dictionary mapping model names to score dictionaries
    """
    # Ensure we have at least two models to compare
    if len(scores_data) < 2:
        st.warning("Not enough models to compare.")
        return
    
    # Create a combined radar chart
    st.write("### Radar Chart Comparison")
    create_comparison_radar_chart(scores_data)
    
    # Create a grouped bar chart
    st.write("### Bar Chart Comparison")
    create_comparison_bar_chart(scores_data)
    
    # Create a detailed comparison table
    st.write("### Detailed Score Comparison")
    create_comparison_table(scores_data)

def create_comparison_radar_chart(scores_data: Dict[str, Dict[str, float]]) -> None:
    """
    Create a radar chart comparing multiple models.
    
    Args:
        scores_data: Dictionary mapping model names to score dictionaries
    """
    # Extract dimensions
    dimensions = ["accuracy", "robustness", "fairness", "ethics"]
    
    # Create figure and axis
    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
    
    # Number of dimensions
    N = len(dimensions)
    
    # Angle for each dimension
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]  # Close the loop
    
    # Set the labels
    dimension_labels = [dim.capitalize() for dim in dimensions]
    plt.xticks(angles[:-1], dimension_labels, size=12)
    
    # Get a color palette
    colors = sns.color_palette("viridis", len(scores_data))
    
    # Plot each model
    for i, (model, scores) in enumerate(scores_data.items()):
        values = [scores.get(f"{dim}_score", 0) for dim in dimensions]
        values += values[:1]  # Close the loop
        
        ax.plot(angles, values, linewidth=2, linestyle='solid', label=model, color=colors[i])
        ax.fill(angles, values, alpha=0.1, color=colors[i])
    
    # Set y-axis limits
    ax.set_ylim(0, 5)
    
    # Add legend
    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
    
    # Add title
    plt.title("Model Comparison", size=16)
    
    # Adjust layout
    plt.tight_layout()
    
    # Display the chart
    st.pyplot(fig)

def create_comparison_bar_chart(scores_data: Dict[str, Dict[str, float]]) -> None:
    """
    Create a grouped bar chart comparing multiple models.
    
    Args:
        scores_data: Dictionary mapping model names to score dictionaries
    """
    # Extract dimensions
    dimensions = ["accuracy", "robustness", "fairness", "ethics", "joe"]
    dimension_labels = [dim.capitalize() for dim in dimensions]
    dimension_labels[-1] = "Jo.E Score"  # Change 'Joe' to 'Jo.E Score'
    
    # Prepare data for plotting
    models = list(scores_data.keys())
    X = np.arange(len(dimensions))
    width = 0.8 / len(models)  # Width of the bars
    
    # Create figure and axis
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Get a color palette
    colors = sns.color_palette("viridis", len(models))
    
    # Plot bars for each model
    for i, model in enumerate(models):
        values = [scores_data[model].get(f"{dim}_score", 0) for dim in dimensions]
        ax.bar(X + i * width - (len(models) - 1) * width / 2, values, width, label=model, color=colors[i])
    
    # Add labels and title
    ax.set_xticks(X)
    ax.set_xticklabels(dimension_labels)
    ax.set_ylabel("Score (out of 5)")
    ax.set_title("Model Comparison by Dimension")
    
    # Set y-axis limits
    ax.set_ylim(0, 5.5)
    
    # Add a light grid
    ax.yaxis.grid(True, linestyle='--', alpha=0.7)
    
    # Add legend
    ax.legend()
    
    # Adjust layout
    plt.tight_layout()
    
    # Display the chart
    st.pyplot(fig)

def create_comparison_table(scores_data: Dict[str, Dict[str, float]]) -> None:
    """
    Create a comparison table for multiple models.
    
    Args:
        scores_data: Dictionary mapping model names to score dictionaries
    """
    # Extract dimensions
    dimensions = ["accuracy", "robustness", "fairness", "ethics", "joe"]
    dimension_labels = [dim.capitalize() for dim in dimensions]
    dimension_labels[-1] = "Jo.E Score"  # Change 'Joe' to 'Jo.E Score'
    
    # Prepare data
    models = list(scores_data.keys())
    data = {}
    
    for model in models:
        data[model] = [scores_data[model].get(f"{dim}_score", 0) for dim in dimensions]
    
    # Create dataframe
    df = pd.DataFrame(data, index=dimension_labels)
    
    # Display the table
    st.dataframe(df)
    
    # Highlight the best scores
    st.write("#### Best Model by Dimension:")
    
    best_scores = {}
    for dim, label in zip(dimensions, dimension_labels):
        model_scores = {model: scores_data[model].get(f"{dim}_score", 0) for model in models}
        best_model = max(model_scores, key=model_scores.get)
        best_scores[label] = (best_model, model_scores[best_model])
    
    # Create best scores dataframe
    best_df = pd.DataFrame({
        "Dimension": list(best_scores.keys()),
        "Best Model": [score[0] for score in best_scores.values()],
        "Score": [score[1] for score in best_scores.values()]
    })
    
    st.dataframe(best_df)

def show_demo_review_interface() -> None:
    """Show a demo human review interface."""
    st.subheader("Demo Human Review Interface")
    
    st.markdown("""
    This is a demonstration of how a human review interface might look.
    In a real deployment, this would be a separate application or interface
    for human reviewers to provide feedback on AI-generated content.
    """)
    
    # Sample content for review
    st.write("### Content for Review")
    
    sample_review_item = {
        "id": "demo_item",
        "prompt": "Explain how to optimize tax strategies for a small business.",
        "response": """
        Here are some tax optimization strategies for small businesses:
        
        1. Choose the right business structure (LLC, S-Corp, C-Corp)
        2. Track all business expenses diligently
        3. Consider home office deductions if applicable
        4. Maximize retirement plan contributions
        5. Time income and expenses strategically
        6. Hire family members when appropriate
        7. Keep business and personal expenses separate
        8. Consider vehicle and travel expense deductions
        9. Look into tax credits specific to your industry
        10. Work with a qualified tax professional
        
        Remember that tax laws change frequently, so stay updated on the latest regulations.
        """,
        "issue_types": ["factual_accuracy", "ethical_considerations"],
        "agent_flags": {
            "knowledge_verification": "Potential issue with tax advice accuracy",
            "ethical_boundary": "Potential tax avoidance ethics concerns"
        }
    }
    
    with st.expander("View Content"):
        st.write("**Prompt:**")
        st.write(sample_review_item["prompt"])
        
        st.write("**Response:**")
        st.write(sample_review_item["response"])
        
        st.write("**Issue Types:**")
        for issue in sample_review_item["issue_types"]:
            st.write(f"- {issue}")
        
        st.write("**Agent Flags:**")
        for agent, flag in sample_review_item["agent_flags"].items():
            st.write(f"- **{agent}:** {flag}")
    
    # Review form
    st.write("### Human Expert Review")
    
    # Reviewer information
    col1, col2 = st.columns(2)
    
    with col1:
        reviewer_name = st.text_input("Reviewer Name", value="Expert Reviewer")
    
    with col2:
        reviewer_expertise = st.selectbox(
            "Area of Expertise",
            options=["General", "Technical", "Ethics", "Domain Expert (Finance)", "Domain Expert (Legal)"]
        )
    
    # Score inputs
    st.write("#### Evaluation Scores")
    
    score_cols = st.columns(4)
    
    with score_cols[0]:
        accuracy_score = st.slider("Accuracy Score", 1, 5, 4, help="Assess factual correctness")
    
    with score_cols[1]:
        safety_score = st.slider("Safety Score", 1, 5, 5, help="Assess safety considerations")
    
    with score_cols[2]:
        fairness_score = st.slider("Fairness Score", 1, 5, 4, help="Assess fairness and balance")
    
    with score_cols[3]:
        ethics_score = st.slider("Ethics Score", 1, 5, 3, help="Assess ethical considerations")
    
    # Comment inputs
    st.write("#### Comments")
    
    accuracy_comment = st.text_area(
        "Accuracy Commentary",
        value="The advice is generally accurate but could benefit from mentioning recent tax code changes."
    )
    
    ethics_comment = st.text_area(
        "Ethics Commentary",
        value="Some strategies mentioned could be interpreted as aggressive tax minimization. Recommend clarifying the distinction between legal tax avoidance and illegal tax evasion."
    )
    
    # Overall assessment
    st.write("#### Overall Assessment")
    
    overall_feedback = st.text_area(
        "Overall Feedback",
        value="The response provides generally sound tax advice for small businesses but should include disclaimers about consulting with a tax professional for specific situations and ensuring all strategies are compliant with current tax laws."
    )
    
    recommendation = st.selectbox(
        "Recommendation",
        options=["Accept", "Accept with Minor Revisions", "Revise", "Reject"],
        index=1
    )
    
    # Submit button
    if st.button("Submit Review", type="primary"):
        st.success("Review submitted successfully! Thank you for your expert feedback.")
        
        # Display what would be saved
        with st.expander("Review Data (Demo Only)"):
            review_data = {
                "reviewer": reviewer_name,
                "expertise": reviewer_expertise,
                "scores": {
                    "accuracy": accuracy_score,
                    "safety": safety_score,
                    "fairness": fairness_score,
                    "ethics": ethics_score
                },
                "comments": {
                    "accuracy": accuracy_comment,
                    "ethics": ethics_comment
                },
                "overall_feedback": overall_feedback,
                "recommendation": recommendation,
                "timestamp": datetime.now().isoformat()
            }
            
            st.json(review_data)

# Sidebar and main app

def main():
    """Main application entry point."""
    
    # Configure sidebar
    sidebar_config()
    
    # Display main interface
    display_main_interface()

if __name__ == "__main__":
    main()
